{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔄 Fine-Tune and Serve a BERT LLM with FFT, LORA, and QLoRa with Union.ai: A Hands-On Tutorial\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/unionai-oss/bert-llm-classification-pipeline/blob/main/tutorial.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "Welcome to this step-by-step tutorial on building a **Large Language Model (LLM) fine-tuning pipeline** using **Hugging Face Transformers, PEFT**  and **Union.ai’s AI workflow and inference platform**. In this tutorial, you’ll fine-tune a **BERT-based model for text classification**, serve it for inference, and track every step of your pipeline using **Union’s MLOps capabilities**.  \n",
    "\n",
    "\n",
    "\n",
    "## 📝 What You'll Build  \n",
    "\n",
    "By the end of this tutorial, you'll have a **fully functional AI pipeline** that:  \n",
    "\n",
    "1. 📥 **Downloads and processes a dataset**   \n",
    "2. 🏋️‍♂️ **Fine-tunes a BERT model for classification with FFT, LORA, and QLoRa**   \n",
    "3. 💾 **Saves and versions the trained model** \n",
    "4. 📊 **Evaluates the model on a test set**   \n",
    "4. 🚀 **Deploys the model for real-time inference**  \n",
    "5. 📈 **Tracks all artifacts and experiments** using Union.ai\n",
    "\n",
    "\n",
    "\n",
    "## 🧰 Setup \n",
    "\n",
    "\n",
    "To get started, sign up for a **Union Serverless** account at [Union.ai](https://union.ai) by clicking the **\"Get Started\"** button. No credit card is required, and you'll receive **$30 in free credits** to begin experimenting. The signup process takes just a few minutes.  \n",
    "\n",
    "Alternatively, if you have access to a **[Union BYOC Enterprise](https://www.union.ai/pricing)** account, you can log into your account.  \n",
    "\n",
    "### 📦 Install Python Packages & Clone Repo\n",
    "\n",
    "Packages can be installed in your local environment using the following command using your preferred package manager from the [requirements.txt](requirements.txt) file. For example `pip install -r requirements.txt`. \n",
    "\n",
    "to clone the repo, run the following command in your environment: `git clone https://github.com/unionai-oss/bert-llm-classification-pipeline`\n",
    "\n",
    "If you're running this notebook in a Google Colab environment, you can install the packages and clone the GitHub repo directly in the notebook by running the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/unionai-oss/bert-llm-classification-pipeline\n",
    "    %cd bert-llm-classification-pipeline\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 🔐 Authenticate\n",
    "To use **Union.ai**, you'll need to authenticate your account. Follow the appropriate step based on your setup:  \n",
    "\n",
    "##### 🔸 **Using Union BYOC Enterprise**  \n",
    "\n",
    "If you're using a **[Union BYOC Enterprise](https://www.union.ai/pricing)** account, log in with the following command:  \n",
    "```bash\n",
    "union create login --host <union-host-url>\n",
    "```\n",
    "\n",
    "Replace <union-host-url> with your organization's Union instance URL.\n",
    "\n",
    "##### 🔸 Using Union Serverless\n",
    "If you're using [Union Serverless](https://www.union.ai/) , authenticate by running the command below:\n",
    "\n",
    "Create an account for free at [Union.ai](https://union.ai) if you don't have one yet:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🌟 Authenticate to union serverless\n",
    "!union create login --serverless --auth device-flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔀 BERT Fine-Tuning Pipeline  \n",
    "We’ll create an **end-to-end machine learning pipeline** to train a **BERT model for text classification** using the **IMDB Review Dataset**.\n",
    "\n",
    "- Run the command below to fine-tune the BERT model using the Union.ai CLI. This command will create a new pipeline and start the training process.\n",
    "\n",
    "- The first time you this command it will take a while to download the model and set up the environment.\n",
    "\n",
    "- The subsequent runs will be faster as the container, model, and data will be cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 👇 Run this command to start the fine-tuning workflow using lora, qlora or full\n",
    "!union run --remote workflows/train_pipeline.py train_pipeline --epochs 3 --tuning_method full "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔎 Explore the Code  \n",
    "\n",
    "- The command above is using files from the [`workflows/`](workflows/train_pipeline.py) and [`tasks`](tasks/) folders that got cloned on setup.\n",
    "\n",
    "- The codeis added to this notebook for reference with the `%%writefile` magic command to overwrite the files if you want to make changes.\n",
    "\n",
    "- You do not need to run the code cells with `%%writefile` unless you want to make changes to the pipeline or tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/train_pipeline.py\n",
    "\n",
    "\"\"\"\n",
    "This file contains the train_pipeline workflow that orchestrates the\n",
    "training pipeline for BERT classification models\n",
    "\"\"\"\n",
    "\n",
    "from union import workflow\n",
    "\n",
    "from tasks.data import download_dataset, visualize_data\n",
    "from tasks.inference import predict_batch_sentiment\n",
    "from tasks.model import download_model, evaluate_model, train_model\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# train pipeline\n",
    "# ---------------------------\n",
    "@workflow\n",
    "def train_pipeline(\n",
    "    tuning_method: str = \"lora\",  # options: \"full\", \"lora\", \"qlora\"\n",
    "    model_name: str = \"distilbert-base-uncased\",\n",
    "    epochs: int = 3,\n",
    "    extra_test_text: list[str] = [\n",
    "        \"This is a great movie!\",\n",
    "        \"This is a bad movie!\",\n",
    "    ],\n",
    ") -> None:\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = download_dataset()\n",
    "    saved_model_dir = download_model(model_name=model_name)\n",
    "\n",
    "    visualize_data(\n",
    "        train_dataset=train_dataset, val_dataset=val_dataset, test_dataset=test_dataset\n",
    "    )\n",
    "\n",
    "    trained_model_dir = train_model(\n",
    "        tuning_method=tuning_method,\n",
    "        model_dir=saved_model_dir,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "\n",
    "    evaluate_model(trained_model_dir=trained_model_dir, test_dataset=test_dataset)\n",
    "\n",
    "    # Perform batch inference\n",
    "    predict_batch_sentiment(trained_model_dir=trained_model_dir, texts=extra_test_text)\n",
    "\n",
    "# Run model training pipeline:\n",
    "#!union run --remote workflows/train_pipeline.py train_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **💡 Note:**  \n",
    "> In more complex ML workflows, **data pipelines** are often separate from **model training pipelines**.  \n",
    "> For simplicity, we'll combine them into a single workflow in this example.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile containers.py\n",
    "\"\"\"\n",
    "This file contains the container image specification for the BERT classification pipeline\n",
    "\"\"\"\n",
    "\n",
    "from flytekit import ImageSpec, Resources\n",
    "\n",
    "container_image = ImageSpec(\n",
    "     name=\"fine-tune-qlora\",\n",
    "    requirements=\"requirements.txt\",\n",
    "    pip_extra_index_url=[\"https://download.pytorch.org/whl/cu118\"],  #enables +cu118 builds\n",
    "    builder=\"union\",\n",
    "    cuda=\"11.8\",  # ensure GPU + CUDA layer is available\n",
    "    apt_packages=[\"gcc\", \"g++\"],  # optional, for packages like bitsandbytes\n",
    ")\n",
    "\n",
    "# we can also define a reusable stateful container environment\n",
    "# See this in action near the end of this notebook for faster bactch inference!\n",
    "actor = ActorEnvironment(\n",
    "    name=\"my-actor\",\n",
    "    container_image=container_image,\n",
    "    replica_count=1,\n",
    "    ttl_seconds=360,\n",
    "    requests=Resources(\n",
    "        cpu=\"2\",\n",
    "        mem=\"5000Mi\",\n",
    "        gpu=\"1\",\n",
    "    ),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "# This file contains the requirements for the BERT classification pipeline\n",
    "\n",
    "torch==2.5.1+cu118\n",
    "transformers==4.48.2\n",
    "datasets>=2.14.0\n",
    "peft==0.14.0\n",
    "bitsandbytes==0.45.3 #change\n",
    "accelerate==1.3.0\n",
    "flytekit==1.15.0\n",
    "union==0.1.151\n",
    "python-dotenv==1.0.1\n",
    "matplotlib==3.10.0\n",
    "pandas==2.2.2\n",
    "scikit-learn==1.6.1\n",
    "seaborn==0.13.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tasks/data.py\n",
    "\n",
    "\"\"\"\n",
    "This module contains tasks for downloading the dataset and visualizing the data.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import Dataset\n",
    "from flytekit.types.directory import FlyteDirectory\n",
    "from flytekit.types.file import FlyteFile\n",
    "from typing_extensions import Annotated\n",
    "from union import Artifact, Deck, Resources, current_context, task\n",
    "\n",
    "from containers import container_image\n",
    "\n",
    "# Define Artifact Specifications\n",
    "RawImdbDataset = Artifact(name=\"raw_imdb_dataset\")\n",
    "TrainImdbDataset = Artifact(name=\"train_imdb_dataset\")\n",
    "ValImdbDataset = Artifact(name=\"val_imdb_dataset\")\n",
    "TestImdbDataset = Artifact(name=\"test_imdb_dataset\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# download dataset\n",
    "# ---------------------------\n",
    "@task(\n",
    "    container_image=container_image,\n",
    "    cache=True,\n",
    "    cache_version=\"1\",\n",
    "    requests=Resources(cpu=\"2\", mem=\"2Gi\"),\n",
    ")\n",
    "def download_dataset() -> tuple[\n",
    "    Annotated[FlyteFile, TrainImdbDataset],\n",
    "    Annotated[FlyteFile, ValImdbDataset],\n",
    "    Annotated[FlyteFile, TestImdbDataset],\n",
    "]:\n",
    "\n",
    "    import pandas as pd\n",
    "    from datasets import load_dataset\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Load IMDB dataset\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    train_df = dataset[\"train\"].to_pandas()\n",
    "    test_df = dataset[\"test\"].to_pandas()\n",
    "\n",
    "    # Split training set into train and validation sets\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_df, test_size=0.2, stratify=train_df[\"label\"], random_state=42\n",
    "    )\n",
    "\n",
    "    working_dir = Path(current_context().working_directory)\n",
    "    data_dir = working_dir / \"data\"\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save datasets as CSV files\n",
    "    train_path = data_dir / \"train.csv\"\n",
    "    val_path = data_dir / \"val.csv\"\n",
    "    test_path = data_dir / \"test.csv\"\n",
    "\n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    val_df.to_csv(val_path, index=False)\n",
    "    test_df.to_csv(test_path, index=False)\n",
    "\n",
    "    return (\n",
    "        TrainImdbDataset.create_from(train_path),\n",
    "        ValImdbDataset.create_from(val_path),\n",
    "        TestImdbDataset.create_from(test_path),\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# visualize data\n",
    "# ---------------------------\n",
    "@task(\n",
    "    container_image=container_image,\n",
    "    enable_deck=True,\n",
    "    requests=Resources(cpu=\"2\", mem=\"2Gi\"),\n",
    ")\n",
    "def visualize_data(\n",
    "    train_dataset: FlyteFile, val_dataset: FlyteFile, test_dataset: FlyteFile\n",
    "):\n",
    "    import base64\n",
    "    from textwrap import dedent\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "\n",
    "    ctx = current_context()\n",
    "\n",
    "    # Load datasets from CSV files\n",
    "    train_df = pd.read_csv(train_dataset.download())\n",
    "    val_df = pd.read_csv(val_dataset.download())\n",
    "    test_df = pd.read_csv(test_dataset.download())\n",
    "\n",
    "    # Create the deck for visualization\n",
    "    deck = Deck(\"Dataset Analysis\")\n",
    "\n",
    "    # Sample reviews from the datasets\n",
    "    train_positive_review = train_df[train_df[\"label\"] == 1].iloc[0][\"text\"]\n",
    "    train_negative_review = train_df[train_df[\"label\"] == 0].iloc[0][\"text\"]\n",
    "    val_positive_review = val_df[val_df[\"label\"] == 1].iloc[0][\"text\"]\n",
    "    val_negative_review = val_df[val_df[\"label\"] == 0].iloc[0][\"text\"]\n",
    "    test_positive_review = test_df[test_df[\"label\"] == 1].iloc[0][\"text\"]\n",
    "    test_negative_review = test_df[test_df[\"label\"] == 0].iloc[0][\"text\"]\n",
    "\n",
    "    # Visualization helper\n",
    "    def plot_label_distribution(df, title, color, output_path):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        df[\"label\"].value_counts().plot(kind=\"bar\", color=color)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Label\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "\n",
    "    # Plot label distributions\n",
    "    plot_label_distribution(\n",
    "        train_df,\n",
    "        \"Train Data Label Distribution\",\n",
    "        \"skyblue\",\n",
    "        \"/tmp/train_label_distribution.png\",\n",
    "    )\n",
    "    plot_label_distribution(\n",
    "        val_df,\n",
    "        \"Validation Data Label Distribution\",\n",
    "        \"orange\",\n",
    "        \"/tmp/val_label_distribution.png\",\n",
    "    )\n",
    "    plot_label_distribution(\n",
    "        test_df,\n",
    "        \"Test Data Label Distribution\",\n",
    "        \"lightgreen\",\n",
    "        \"/tmp/test_label_distribution.png\",\n",
    "    )\n",
    "\n",
    "    # Convert images to base64 for embedding\n",
    "    def image_to_base64(image_path):\n",
    "        with open(image_path, \"rb\") as img_file:\n",
    "            return base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "\n",
    "    train_image_base64 = image_to_base64(\"/tmp/train_label_distribution.png\")\n",
    "    val_image_base64 = image_to_base64(\"/tmp/val_label_distribution.png\")\n",
    "    test_image_base64 = image_to_base64(\"/tmp/test_label_distribution.png\")\n",
    "\n",
    "    # Create HTML report\n",
    "    html_report = dedent(\n",
    "        f\"\"\"\n",
    "    <div style=\"font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "        <h2 style=\"color: #2C3E50;\">Dataset Analysis</h2>\n",
    "\n",
    "        <h3 style=\"color: #2980B9;\">Training Data Summary</h3>\n",
    "        <img src=\"data:image/png;base64,{train_image_base64}\" alt=\"Train Data Label Distribution\" width=\"600\">\n",
    "        Shape: {train_df.shape} <br>\n",
    "        Label Distribution: {train_df['label'].value_counts()} <br>\n",
    "        <p><strong>Positive Review:</strong> {train_positive_review}</p>\n",
    "        <p><strong>Negative Review:</strong> {train_negative_review}</p>\n",
    "\n",
    "        <h3 style=\"color: #2980B9;\">Validation Data Summary</h3>\n",
    "        <img src=\"data:image/png;base64,{val_image_base64}\" alt=\"Validation Data Label Distribution\" width=\"600\">\n",
    "        Shape: {val_df.shape} <br>\n",
    "        Label Distribution: {val_df['label'].value_counts()} <br>\n",
    "        <p><strong>Positive Review:</strong> {val_positive_review}</p>\n",
    "        <p><strong>Negative Review:</strong> {val_negative_review}</p>\n",
    "\n",
    "        <h3 style=\"color: #2980B9;\">Test Data Summary</h3>\n",
    "        <img src=\"data:image/png;base64,{test_image_base64}\" alt=\"Test Data Label Distribution\" width=\"600\">\n",
    "        Shape: {test_df.shape} <br>\n",
    "        Label Distribution: {test_df['label'].value_counts()} <br>\n",
    "        <p><strong>Positive Review:</strong> {test_positive_review}</p>\n",
    "        <p><strong>Negative Review:</strong> {test_negative_review}</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # Append HTML content to the deck\n",
    "    deck.append(html_report)\n",
    "\n",
    "    # Insert the deck into the context\n",
    "    ctx.decks.insert(0, deck)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tasks/model.py\n",
    "\"\"\"\n",
    "This file contains the tasks that are used to download, train and evaluate the model.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from flytekit.types.directory import FlyteDirectory\n",
    "from flytekit.types.file import FlyteFile\n",
    "from typing_extensions import Annotated\n",
    "from union import Artifact, Deck, Resources, current_context, task\n",
    "from containers import container_image\n",
    "\n",
    "# Define Artifact Specifications\n",
    "FineTunedImdbModel = Artifact(name=\"fine_tuned_Imdb_model\")\n",
    "\n",
    "# ---------------------------\n",
    "# download model\n",
    "# ---------------------------\n",
    "@task(\n",
    "    container_image=container_image,\n",
    "    cache=True,\n",
    "    cache_version=\"1\",\n",
    "    requests=Resources(cpu=\"2\", mem=\"2Gi\"),\n",
    ")\n",
    "def download_model(model_name: str) -> FlyteDirectory:\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "    working_dir = Path(current_context().working_directory)\n",
    "    saved_model_dir = working_dir / \"saved_model\"\n",
    "    saved_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    # update AutoModelForSequenceClassification to \"AutoModelForCausalLM\" for causal models\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"cpu\",\n",
    "        torch_dtype=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model.save_pretrained(saved_model_dir)\n",
    "    tokenizer.save_pretrained(saved_model_dir)\n",
    "\n",
    "    return FlyteDirectory(saved_model_dir)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# full/lora/qlora fine-tune model\n",
    "# ---------------------------\n",
    "@task(\n",
    "    container_image=container_image,\n",
    "    requests=Resources(cpu=\"4\", mem=\"12Gi\", gpu=\"1\"),\n",
    ")\n",
    "def train_model(\n",
    "    model_dir: FlyteDirectory,\n",
    "    train_dataset: FlyteFile,\n",
    "    val_dataset: FlyteFile,\n",
    "    epochs: int = 3,\n",
    "    tuning_method: str = \"full\",  # options: \"full\", \"lora\", \"qlora\"\n",
    "    lora_r: int = 8,\n",
    "    lora_alpha: int = 16,\n",
    "    lora_dropout: float = 0.1,\n",
    ") -> Annotated[FlyteDirectory, FineTunedImdbModel]:\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    from datasets import Dataset\n",
    "    from transformers import (\n",
    "        AutoModelForSequenceClassification,\n",
    "        AutoTokenizer,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "    )\n",
    "\n",
    "    # Load datasets\n",
    "    #------------------------------------\n",
    "    local_model_dir = model_dir.download()\n",
    "    train_df = pd.read_csv(train_dataset.download()).sample(n=500, random_state=42)\n",
    "    val_df = pd.read_csv(val_dataset.download()).sample(n=100, random_state=42)\n",
    "\n",
    "    train_dataset_hf = Dataset.from_pandas(train_df)\n",
    "    val_dataset_hf = Dataset.from_pandas(val_df)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_model_dir)\n",
    "\n",
    "    def tokenizer_function(example):\n",
    "        return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "    tokenized_train = train_dataset_hf.map(tokenizer_function)\n",
    "    tokenized_val = val_dataset_hf.map(tokenizer_function)\n",
    "\n",
    "    # Load & Setup Model\n",
    "    #------------------------------------\n",
    "    # qlora will load the model in 4-bit quantization\n",
    "    if tuning_method == \"qlora\":\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            llm_int8_skip_modules=[\"classifier\", \"pre_classifier\"],\n",
    "        )\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            local_model_dir,\n",
    "            quantization_config=bnb_config,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            # device_map=\"auto\", # use this for models if implemented\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # Load the model normally\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(local_model_dir)\n",
    "\n",
    "    # if lora or qlora, set the LoRA config\n",
    "    if tuning_method in {\"lora\", \"qlora\"}:\n",
    "        from peft import get_peft_model, LoraConfig, TaskType\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            target_modules=[\"q_lin\", \"k_lin\", \"v_lin\"], # query, Key, Value linear layers in this model\n",
    "        )\n",
    " \n",
    "        model = get_peft_model(model, lora_config)\n",
    "\n",
    "    \n",
    "    # Model fine-tuning\n",
    "    #------------------------------------\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    #------------------------------------\n",
    "    # Merge LoRA weights into base model (you could also just save adapter weights)\n",
    "    if tuning_method in {\"lora\", \"qlora\"}:\n",
    "        model = model.merge_and_unload()\n",
    "\n",
    "    output_dir = Path(current_context().working_directory) / \"trained_model\"\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    #TODO: Save traning type (lora, qlora, full) as artifacts\n",
    "    return FineTunedImdbModel.create_from(output_dir)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# evaluate model\n",
    "# ---------------------------\n",
    "@task(\n",
    "    container_image=container_image,\n",
    "    enable_deck=True,\n",
    "    requests=Resources(cpu=\"2\", mem=\"12Gi\", gpu=\"1\"),\n",
    ")\n",
    "def evaluate_model(trained_model_dir: FlyteDirectory, test_dataset: FlyteFile) -> dict:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datasets import Dataset\n",
    "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "    from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import base64\n",
    "    from union import current_context\n",
    "    from union import Deck\n",
    "    from textwrap import dedent\n",
    "\n",
    "    # Download model locally\n",
    "    local_model_dir = trained_model_dir.download()\n",
    "    ctx = current_context()\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        local_model_dir,\n",
    "        torch_dtype=\"auto\",\n",
    "        load_in_4bit=False,  # Important: for evaluation, avoid loading in quantized 4-bit unless you really want to\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_model_dir)\n",
    "\n",
    "    # Load and prepare the test dataset\n",
    "    test_df = pd.read_csv(test_dataset.download()).sample(n=100, random_state=42)\n",
    "\n",
    "    # Use a pipeline for evaluation (bypasses Trainer and works for quantized models)\n",
    "    nlp_pipeline = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        # device=0 if torch.cuda.is_available() else -1,  # auto-select device\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    # Perform batch inference\n",
    "    predictions = nlp_pipeline(test_df[\"text\"].tolist(), batch_size=8)\n",
    "\n",
    "    # Extract predicted labels\n",
    "    pred_labels = [int(p[\"label\"].split(\"_\")[-1]) if \"label\" in p else 0 for p in predictions]\n",
    "    true_labels = test_df[\"label\"].tolist()\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(true_labels, pred_labels),\n",
    "        \"f1\": f1_score(true_labels, pred_labels, average=\"weighted\"),\n",
    "        \"precision\": precision_score(true_labels, pred_labels, average=\"weighted\"),\n",
    "        \"recall\": recall_score(true_labels, pred_labels, average=\"weighted\"),\n",
    "        # \"conf_matrix\": confusion_matrix(true_labels, pred_labels)\n",
    "    }\n",
    "\n",
    "    # create visualization deck\n",
    "    deck = Deck(\"Model Evaluation\")\n",
    "\n",
    "    # Generate Confusion Matrix\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    cm_path = f\"/tmp/confusion_matrix.png\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=sorted(set(true_labels)), yticklabels=sorted(set(true_labels)))\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(cm_path)\n",
    "    plt.close()\n",
    "    \n",
    "    # Convert images to base64 for embedding\n",
    "    def image_to_base64(image_path):\n",
    "        with open(image_path, \"rb\") as img_file:\n",
    "            return base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "        \n",
    "    cm_image_base64 = image_to_base64(cm_path)\n",
    "\n",
    "    # Create HTML report\n",
    "    html_report = dedent(\n",
    "        f\"\"\"\n",
    "    <div style=\"font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "        <h2 style=\"color: #2C3E50;\">Model Evaluation</h2>\n",
    "\n",
    "        <h3 style=\"color: #2980B9;\">Confusion Matrix</h3>\n",
    "        <img src=\"data:image/png;base64,{cm_image_base64}\" alt=\"Confusion Matrix\" width=\"600\">\n",
    "        <h3 style=\"color: #2980B9;\">Model Metrics</h3>\n",
    "        <pre>{metrics}</pre>\n",
    "        \n",
    "    </div>\n",
    "        \"\"\")\n",
    "\n",
    "     # Append HTML content to the deck\n",
    "    deck.append(html_report)\n",
    "    # Insert the deck into the context\n",
    "    ctx.decks.insert(0, deck)\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Serving the Fine-Tuned BERT model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live App Serving (Beta)\n",
    "\n",
    "Union.ai provides a **simple way to serve your models as a live app**, making it easy to interact with your trained model.  \n",
    "\n",
    "In this example, we'll deploy the model using **Streamlit**, which provides a **simple web interface** for running predictions.  \n",
    "\n",
    "\n",
    "📂 Check out the following files for the model-serving code:  \n",
    "-[`app.py`](app.py) – Handles **loading the model** and serving it via Union.ai.  \n",
    "- [`main.py`](main.py) – Defines the **Streamlit-based UI** for interacting with the model.  \n",
    "\n",
    "Deploy the model by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 👇 Run this command to serve the model & streamlit application\n",
    "!union deploy apps app.py bert-sentiment-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the training pipeline, the code is added to this notebook for reference with the `%%writefile` magic command to overwrite the files if you want to make changes directly in the notebook. But running the cells below are not required since the code is already in the `workflows/` and `tasks/` folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\"\"\"A Union app that uses hugging face and Streamlit\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from union import Artifact, ImageSpec, Resources\n",
    "from union.app import App, Input, ScalingMetric\n",
    "from datetime import timedelta\n",
    "from flytekit.extras.accelerators import L4, GPUAccelerator\n",
    "\n",
    "\n",
    "# Define the artifact that holds the BERT model.\n",
    "FineTunedImdbModel = Artifact(name=\"fine_tuned_Imdb_model\")\n",
    "\n",
    "# Define the container image including the required packages.\n",
    "# ---------------------------------------\n",
    "image_spec = ImageSpec(\n",
    "    name=\"union-serve-bert-sentiment-analysis\",\n",
    "    packages=[\n",
    "        \"transformers==4.48.3\",\n",
    "        \"union-runtime>=0.1.11\",\n",
    "        \"accelerate==1.5.2\",\n",
    "        \"streamlit==1.43.2\",\n",
    "        \"bitsandbytes==0.45.3\"\n",
    "    ],\n",
    "    builder=\"union\",\n",
    "    registry=os.getenv(\"REGISTRY\"),\n",
    ")\n",
    "\n",
    "# Create the Union Serving App.\n",
    "# ---------------------------------------\n",
    "streamlit_app = App(\n",
    "    name=\"bert-sentiment-analysis\",\n",
    "    inputs=[\n",
    "        Input(\n",
    "            name=\"bert_model\",\n",
    "            value=FineTunedImdbModel.query(),\n",
    "            download=True,  # The model artifact is downloaded when the container starts.\n",
    "        )\n",
    "    ],\n",
    "    container_image=image_spec,\n",
    "    limits=Resources(cpu=\"2\", mem=\"24Gi\", gpu=\"1\", ephemeral_storage=\"20Gi\"),\n",
    "    requests=Resources(cpu=\"2\", mem=\"24Gi\", gpu=\"1\", ephemeral_storage=\"20Gi\"),\n",
    "    accelerator=L4,\n",
    "    port=8082,\n",
    "    include=[\"./main.py\"],  # Include your Streamlit code.\n",
    "    args=[\"streamlit\", \"run\", \"main.py\", \"--server.port\", \"8082\"],\n",
    "    min_replicas=0,\n",
    "    max_replicas=1,\n",
    "    scaledown_after=timedelta(minutes=5),\n",
    "    scaling_metric=ScalingMetric.Concurrency(2),\n",
    "    # requires_auth=False # Uncomment to make app public.\n",
    ")\n",
    "\n",
    "# union deploy apps app.py bert-sentiment-analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "\"\"\"\n",
    "A simple Union app using Streamlit to serve a BERT model with Streamlit.\n",
    "\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from union_runtime import get_input\n",
    "\n",
    "# Load the model artifact downloaded by Union.\n",
    "# ---------------------------------------\n",
    "model_path = get_input(\"bert_model\")\n",
    "try:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "except Exception as e:\n",
    "    st.error(f\"Error loading model: {e}\")\n",
    "    st.stop()\n",
    "\n",
    "\n",
    "# Creat e the Streamlit app.\n",
    "# ---------------------------------------\n",
    "st.title(\"Sentiment Analyzer\")\n",
    "st.write(\"Enter text to predict the sentiment.\")\n",
    "\n",
    "# Input text for sentiment analysis\n",
    "user_input = st.text_area(\"Enter your text:\", height=400, key=\"text_input\")\n",
    "\n",
    "if st.button(\"Analyze\"):\n",
    "    try:\n",
    "        # Tokenize and predict\n",
    "        inputs = tokenizer(\n",
    "            user_input, return_tensors=\"pt\", truncation=True, padding=True\n",
    "        )\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        labels = [\"NEGATIVE\", \"POSITIVE\"]  # Adjust according to your model's labels\n",
    "\n",
    "        sentiment = labels[predictions.item()]\n",
    "        score = probabilities[0][predictions.item()].item()\n",
    "\n",
    "        if sentiment == \"NEGATIVE\":\n",
    "            st.error(f\"Predicted sentiment: {sentiment} (Confidence: {score:.2f})\")\n",
    "        else:\n",
    "            st.success(f\"Predicted sentiment: {sentiment} (Confidence: {score:.2f})\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"Prediction error: {e}\")\n",
    "\n",
    "# union deploy apps app.py bert-sentiment-analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Union platform `Apps` tab to see the status of all apps!\n",
    "\n",
    "Once the app is live, experiment with different inputs and see how your fine-tuned BERT model performs! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Optional: Use workflows to run batch inference\n",
    "\n",
    "### Batch Serving\n",
    "\n",
    "Union.ai also provides a way to serve your models in batch mode. This is useful when you have a large number of predictions to make and you want to do them all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 👇 Run this command to register tasks & workflows on Union.ai\n",
    "!union register workflows/batch_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below we'll use union remote to run the batch inference pipeline directly in the Notebook. \n",
    "- This will create a new pipeline and start the batch inference process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile containers.py\n",
    "\n",
    "from flytekit import ImageSpec, Resources\n",
    "from union.actor import ActorEnvironment\n",
    "\n",
    "container_image = ImageSpec(\n",
    "     name=\"fine-tune-qlora\",\n",
    "    requirements=\"requirements.txt\",\n",
    "    pip_extra_index_url=[\"https://download.pytorch.org/whl/cu118\"],  #enables +cu118 builds\n",
    "    builder=\"union\",\n",
    "    cuda=\"11.8\",  # ensure GPU + CUDA layer is available\n",
    "    apt_packages=[\"gcc\", \"g++\"],  # optional, for packages like bitsandbytes\n",
    ")\n",
    "\n",
    "actor = ActorEnvironment(\n",
    "    name=\"my-actor\",\n",
    "    container_image=container_image,\n",
    "    replica_count=1,\n",
    "    ttl_seconds=360,\n",
    "    requests=Resources(\n",
    "        cpu=\"2\",\n",
    "        mem=\"5000Mi\",\n",
    "        gpu=\"1\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from union.remote import UnionRemote\n",
    "# Create a remote connection\n",
    "remote = UnionRemote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_container(data):\n",
    "\n",
    "    inputs = {\"texts\": data}\n",
    "\n",
    "    workflow = remote.fetch_workflow(name=\"workflows.batch_inference.batch_inference_workflow\")\n",
    "    execution = remote.execute(workflow, inputs=inputs, wait=True) # wait=True will block until the execution is complete\n",
    "\n",
    "    # print(execution.outputs)\n",
    "\n",
    "    return execution.outputs[\"o0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_with_container([\"I love this movie\",\n",
    "                               \"I hate this movie\"]\n",
    "                               ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚡ Faster batch serving with Union Actors\n",
    "\n",
    "Union [Actors](https://docs.union.ai/serverless/user-guide/core-concepts/actors/#actors) dramatically reduce the cost of cold starts by maintaining long-running stateful environments that stay ready for use until a defined time-to-live (TTL). This persistent setup eliminates redundant initialization and unlocks several key benefits. This can be especially useful for AI pipelines that benefit from long-running environments, such as large containers, serving models,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_actor(data):\n",
    "\n",
    "    inputs = {\"texts\": data}\n",
    "\n",
    "    workflow = remote.fetch_workflow(name=\"workflows.batch_inference.actor_batch_inference_workflow\")\n",
    "    execution = remote.execute(workflow, inputs=inputs, wait=True) # wait=True will block until the execution is complete\n",
    "\n",
    "    # print(execution.outputs)\n",
    "\n",
    "    return execution.outputs['o0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run the next three commands in quick succession to see the actor container in action. \n",
    "- After the first command, the actor will be created and will stay alive for 5 minutes after the last call. \n",
    "- The commands after the first will be run in the same actor container, so you should see a significant speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_with_actor([\"I love this movie\",\n",
    "                               \"I hate this movie\"]\n",
    "                               ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_with_actor([\"I love this movie\",\n",
    "                               \"I hate this movie\"]\n",
    "                               ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_with_actor([\"I love this movie\",\n",
    "                               \"I hate this movie\"]\n",
    "                               ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources to learn more\n",
    "- Union.ai & Flyte Documentation: https://www.union.ai/docs/byoc/user-guide/\n",
    "- Building AI Together Slack: https://slack.flyte.org/\n",
    "- Flyte Github: https://github.com/flyteorg/flyte\n",
    "- Union.ai OSS Github: https://github.com/unionai-oss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
