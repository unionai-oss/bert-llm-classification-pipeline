{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ Fine-Tune and Serve a BERT Model (LLM)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/unionai-oss/bert-llm-classification-pipeline/blob/main/tutorial.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This tutorial will walk you through building an end-to-end Large Language Model (LLM) fine-tuning pipeline using Hugging Face Transformers and Union's AI workflow and inference platform. We'll download a dataset, fine-tune a BERT model for classification on unstructured data, serve the model for inference, and track the pipeline artifacts using Union's powerful MLOps features. Although this example may seem relatively simple, all the concepts and tools used here can be applied to more complex machine learning and AI projects.\n",
    "\n",
    "\n",
    "By just adding a few lines of code to your Python functions, you'll be able to create a reproducible ML pipeline, taking advantage of Union's features:\n",
    "\n",
    "- Reproducible AI workflows: Ensure your ML pipeline produces the same environments every time.\n",
    "- Versioning of code and artifacts: Track changes in your code and models automatically.\n",
    "- Data Caching for faster iterations: Reuse results from previous executions to save time.\n",
    "- Declarative Infrastructure: Define your ML infrastructure needs directly in your code without worrying about provisioning.\n",
    "- Artifact Management for models and data: Automatically manage your model files and datasets.\n",
    "- Container Image Builder: Build and deploy your code in a consistent environment.\n",
    "- Local Development: Test your workflows locally before deploying them to the cloud.\n",
    "- Actors for long-running stateful containers: Handle tasks that require continuous state or interaction.\n",
    "- And more...\n",
    "\n",
    "Example of how to use Union's Python SDK to define a simple ML pipeline:\n",
    "\n",
    "```python\n",
    "@task(\n",
    "    cache=True,\n",
    "    cache_version=\"4\",\n",
    "    container_image=image,\n",
    "    requests=Resources(cpu=\"2\", mem=\"2Gi\")\n",
    ")\n",
    "def download_data(): -> pd.DataFrame:\n",
    "    ...\n",
    "\n",
    "@task(\n",
    "    container_image=image,\n",
    "    requests=Resources(cpu=\"2\", mem=\"20Gi\", gpu=\"1\")\n",
    ")\n",
    "def train_model(data: pd.DataFrame:): -> pytorch.Model:\n",
    "    ...\n",
    "\n",
    "@workflow()\n",
    "def pipeline_workflow():\n",
    "    data = download_data()\n",
    "    train_model(data=data)\n",
    "    ...\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## üß∞ Setup \n",
    "\n",
    "Sign up for a Union Serverless account at [Union.ai](https://union.ai) by clicking the \"Get Started\" button. No card required, and you'll get $30 in free credits to get started. Signing up can take a few minutes.\n",
    "\n",
    "Or you can use your [Union BYOC Enterprise](https://www.union.ai/pricing) login if you have one.\n",
    "\n",
    "### üì¶ Install Python Packages & Clone Repo\n",
    "\n",
    "Packages can be installed in your local environment using the following command using your preferred package manager from the [requirements.txt](requirements.txt) file. For example `pip install -r requirements.txt`. \n",
    "\n",
    "to clone the repo, run the following command in your environment: `git clone `\n",
    "\n",
    "If you're running this notebook in a Google Colab environment, you can install the packages and clone the GitHub repo directly in the notebook by running the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/unionai-oss/bert-llm-classification-pipeline\n",
    "    %cd bert-llm-classification-pipeline\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### üîê Authenticate\n",
    "\n",
    "If you're using [Union BYOC Enterprise](https://www.union.ai/pricing) use: `union create login --host <union-host-url>`\n",
    "\n",
    "Otherwise, Authenticate to [Union Serverless](https://www.union.ai/) by running the command below - create an account for free at [Union.ai](https://union.ai) if you don't have one:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîê \u001b[33mConfiguration saved to \u001b[0m\u001b[33m/Users/sageelliott/.union/\u001b[0m\u001b[33mconfig.yaml\u001b[0m\n",
      "Login successful into \u001b[1;32mserverless\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!union create login --serverless --auth device-flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÄ BERT Fine-Tuning Pipeline\n",
    "\n",
    "In this sections we'll be running tasks and workflows defined in Python under the relevant folders. \n",
    "\n",
    "Navigate to the `tasks` and `workflows` folders to see the code. if you're following along in a hosted jupyter notebook you should be able to view the code by clicking on a folder icon (usually on the left side of the screen).\n",
    "\n",
    "First we'll create a machine learning pipeline that trains a model on the iris dataset.\n",
    "\n",
    "Our workflow will have the following steps:\n",
    "- Download Dataset & Preprocess\n",
    "- Download BERT Model\n",
    "- Fine-tune BERT Model\n",
    "- Evaluate the model\n",
    "- Save model as an artifact (We'll serve the model in the next section)\n",
    "- Run a prediction with new test data\n",
    "\n",
    "Note: Data pipelines could be seperate from model training pipelines for more complex pipelines. In this example we'll keep it simple and combine them into one workflow.\n",
    "\n",
    "navigate to the [workflows/workflows.py](workflows/train_pipeline.py) file. Find `train_pipeline()` function to see the code for the workflow. This workflow uses tasks defined in the [/tasks](tasks/data.py) folder and builds a container image from [container.py](containers.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run --remote workflows/train_pipeline.py train_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Serving the Fine-Tuned BERT model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live App Serving (Beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union deploy apps app.py bert-sentiment-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mRunning pyflyte register from /Users/sageelliott/Documents/gitrepos/tut-bert-hf with images ImageConfig(default_image=Image(name='default', fqn='cr.union.ai/v1/unionai/union', tag='py3.11-0.1.142', digest=None), images=[Image(name='default', fqn='cr.union.ai/v1/unionai/union', tag='py3.11-0.1.142', digest=None)]) and image destination folder /root on 1 package(s) ('/Users/sageelliott/Documents/gitrepos/tut-bert-hf/workflows/batch_inference.py',)\u001b[0m\n",
      "Registering against serverless-1.us-east-2.s.union.ai\u001b[0m\n",
      "\u001b[33mDetected Root /Users/sageelliott/Documents/gitrepos/tut-bert-hf, using this to create deployable package...\u001b[0m\n",
      "\u001b[33mLoading packages ['workflows.batch_inference'] under source root /Users/sageelliott/Documents/gitrepos/tut-bert-hf\u001b[0m\n",
      "\u001b[33mNo output path provided, using a temporary directory at /var/folders/nv/hcrpygqd6xvd6m2cf6w3pbvc0000gn/T/tmpgpen8_qr instead\u001b[0m\n",
      "\u001b[33mComputed version is hfFIHDHk88f_AR_c4CKprA\u001b[0m\n",
      "\u001b[34mImage flytekit:pQPzS42lZRRflp2iFhumig found. Skip building.\u001b[0m\n",
      "\u001b[32mSerializing and registering 3 flyte entities\u001b[0m\n",
      "\u001b[1;32m[\u001b[0m\u001b[32m‚úî\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m Task: \u001b[0m\u001b]8;id=42188;https://serverless.union.ai/org/sagecodes/projects/default/domains/development/task/tasks.inference.predict_batch_sentiment/version/hfFIHDHk88f_AR_c4CKprA\u001b\\\u001b[4;36mtasks.inference.predict_batch_sentiment\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[1;32m[\u001b[0m\u001b[32m‚úî\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m Workflow: \u001b[0m\u001b]8;id=125000;https://serverless.union.ai/org/sagecodes/projects/default/domains/development/workflow/workflows.batch_inference.batch_inference_workflow/version/hfFIHDHk88f_AR_c4CKprA\u001b\\\u001b[4;36mworkflows.batch_inference.batch_inference_workflow\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[1;32m[\u001b[0m\u001b[32m‚úî\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m Launch Plan: \u001b[0m\u001b]8;id=837698;https://serverless.union.ai/org/sagecodes/projects/default/domains/development/launch_plan/workflows.batch_inference.batch_inference_workflow/version/hfFIHDHk88f_AR_c4CKprA\u001b\\\u001b[4;36mworkflows.batch_inference.batch_inference_workflow\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[32mSuccessfully registered 3 entities\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!union register workflows/batch_inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">10:22:28.110025 </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> remote.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">286</span> - Jupyter notebook and interactive task  \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                </span>         support is still alpha.                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m10:22:28.110025\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m remote.py:\u001b[1;36m286\u001b[0m - Jupyter notebook and interactive task  \n",
       "\u001b[2;36m                \u001b[0m         support is still alpha.                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from union.remote import UnionRemote\n",
    "# Create a remote connection\n",
    "remote = UnionRemote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_container(data):\n",
    "\n",
    "    inputs = {\"texts\": data}\n",
    "\n",
    "    workflow = remote.fetch_workflow(name=\"workflows.batch_inference.batch_inference_workflow\")\n",
    "    execution = remote.execute(workflow, inputs=inputs, wait=True) # wait=True will block until the execution is complete\n",
    "\n",
    "    print(execution.outputs)\n",
    "\n",
    "    return execution.outputs[\"o0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o0': [b'gqVsYWJlbKdMQUJFTF8xpXNjb3Jlyz/sIz+AAAAA', b'gqVsYWJlbKdMQUJFTF8wpXNjb3Jlyz/lTCjgAAAA']}\n",
      "[{'label': 'LABEL_1', 'score': 0.8793027400970459}, {'label': 'LABEL_0', 'score': 0.6655468344688416}]\n"
     ]
    }
   ],
   "source": [
    "print(predict_with_container([\"I love this movie\",\n",
    "                               \"I hate this movie\"]\n",
    "                               ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Faster batch serving with Union Actors\n",
    "\n",
    "Union [Actors](https://docs.union.ai/serverless/user-guide/core-concepts/actors/#actors) dramatically reduce the cost of cold starts by maintaining long-running stateful environments that stay ready for use until a defined time-to-live (TTL). This persistent setup eliminates redundant initialization and unlocks several key benefits. This can be especially useful for AI pipelines that benefit from long-running environments, such as large containers, serving models,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_actor(data):\n",
    "\n",
    "    inputs = {\"texts\": data}\n",
    "\n",
    "    workflow = remote.fetch_workflow(name=\"workflows.batch_inference.batch_inference_workflow\")\n",
    "    execution = remote.execute(workflow, inputs=inputs, wait=True) # wait=True will block until the execution is complete\n",
    "\n",
    "    # print(execution.outputs)\n",
    "\n",
    "    return execution.outputs['o0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_with_actor([[5.1, 3.5, 1.4, 0.2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
