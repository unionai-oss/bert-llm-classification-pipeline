{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ Fine-Tune and Serve LLMs with Union.ai: A Hands-On Tutorial\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/unionai-oss/bert-llm-classification-pipeline/blob/main/tutorial.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "Welcome to this step-by-step tutorial on building a **Large Language Model (LLM) fine-tuning pipeline** using **Hugging Face Transformers** and **Union.ai‚Äôs AI workflow and inference platform**. In this tutorial, you‚Äôll train a **BERT-based model for text classification**, serve it for inference, and track every step of your pipeline using **Union‚Äôs powerful MLOps capabilities**.  \n",
    "\n",
    "This example might seem simple, but the **core concepts and tools** covered here apply to real-world **AI and machine learning (ML) projects** at any scale. By following along, you'll gain hands-on experience in:  \n",
    "\n",
    "üî∏ **Automating ML workflows** with Union.ai  \n",
    "üî∏ **Fine-tuning a transformer model** with Hugging Face  \n",
    "üî∏ **Deploying a model for inference** and tracking artifacts  \n",
    "üî∏ **Optimizing your pipeline** with caching and versioning  \n",
    "\n",
    "## ‚ú® Why Use Union.ai?  \n",
    "\n",
    "By just adding a few lines of code to your Python functions, you'll be able to create a reproducible ML pipeline, taking advantage of Union's features:\n",
    "\n",
    "With just **a few lines of code**, you can transform your Python functions into **scalable, reproducible AI workflows**. Here‚Äôs what you get:  \n",
    "\n",
    "- **üõ† Reproducible AI Workflows** ‚Äì Ensure your pipeline runs in the same environment every time.  \n",
    "- **üìå Versioning & Tracking** ‚Äì Automatically track **code, models, and artifacts**.  \n",
    "- **‚ö° Faster Iterations with Data Caching** ‚Äì Reuse previous results to speed up experiments.  \n",
    "- **üñ• Declarative Infrastructure** ‚Äì Define ML infrastructure **in code** without manual setup.  \n",
    "- **üìÇ Artifact Management** ‚Äì Keep track of model checkpoints and datasets seamlessly.  \n",
    "- **üì¶ Containerized Execution** ‚Äì Deploy models in a consistent environment with automatic **image building**.  \n",
    "- **üßë‚Äçüíª Local & Cloud Development** ‚Äì Test locally before scaling up.  \n",
    "- **üé≠ Actors for Long Running Stateful Containers** ‚Äì Run **Effceint batch inference** with persistent containers.  \n",
    "- **‚Ä¶and much more!** \n",
    "\n",
    "## üìù What You'll Build  \n",
    "\n",
    "By the end of this tutorial, you'll have a **fully functional AI pipeline** that:  \n",
    "\n",
    "1. **Downloads and processes a dataset** üì•  \n",
    "2. **Fine-tunes a BERT model for classification** üèãÔ∏è‚Äç‚ôÇÔ∏è  \n",
    "3. **Saves and versions the trained model** üíæ  \n",
    "4. **Deploys the model for real-time inference** üöÄ  \n",
    "5. **Tracks all artifacts and experiments** using Union.ai üìä  \n",
    "\n",
    "Let‚Äôs dive in! Here's a sneak peek at how simple it is to define a **Union-powered ML pipeline**: \n",
    "\n",
    "```python\n",
    "@task(\n",
    "    cache=True,\n",
    "    cache_version=\"4\",\n",
    "    container_image=image,\n",
    "    requests=Resources(cpu=\"2\", mem=\"2Gi\")\n",
    ")\n",
    "def download_data(): -> pd.DataFrame:\n",
    "    ...\n",
    "\n",
    "@task(\n",
    "    container_image=image,\n",
    "    requests=Resources(cpu=\"2\", mem=\"20Gi\", gpu=\"1\")\n",
    ")\n",
    "def train_model(data: pd.DataFrame:): -> pytorch.Model:\n",
    "    ...\n",
    "\n",
    "@workflow()\n",
    "def pipeline_workflow():\n",
    "    data = download_data()\n",
    "    train_model(data=data)\n",
    "    ...\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## üß∞ Setup \n",
    "\n",
    "\n",
    "To get started, sign up for a **Union Serverless** account at [Union.ai](https://union.ai) by clicking the **\"Get Started\"** button. No credit card is required, and you'll receive **$30 in free credits** to begin experimenting. The signup process takes just a few minutes.  \n",
    "\n",
    "Alternatively, if you have access to a **[Union BYOC Enterprise](https://www.union.ai/pricing)** account, you can log into your account.  \n",
    "\n",
    "### üì¶ Install Python Packages & Clone Repo\n",
    "\n",
    "Packages can be installed in your local environment using the following command using your preferred package manager from the [requirements.txt](requirements.txt) file. For example `pip install -r requirements.txt`. \n",
    "\n",
    "to clone the repo, run the following command in your environment: `git clone https://github.com/unionai-oss/bert-llm-classification-pipeline`\n",
    "\n",
    "If you're running this notebook in a Google Colab environment, you can install the packages and clone the GitHub repo directly in the notebook by running the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/unionai-oss/bert-llm-classification-pipeline\n",
    "    %cd bert-llm-classification-pipeline\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### üîê Authenticate\n",
    "To use **Union.ai**, you'll need to authenticate your account. Follow the appropriate step based on your setup:  \n",
    "\n",
    "##### üî∏ **Using Union BYOC Enterprise**  \n",
    "\n",
    "If you're using a **[Union BYOC Enterprise](https://www.union.ai/pricing)** account, log in with the following command:  \n",
    "```bash\n",
    "union create login --host <union-host-url>\n",
    "```\n",
    "\n",
    "Replace <union-host-url> with your organization's Union instance URL.\n",
    "\n",
    "##### üî∏ Using Union Serverless\n",
    "If you're using [Union Serverless](https://www.union.ai/) , authenticate by running the command below:\n",
    "\n",
    "Create an account for free at [Union.ai](https://union.ai) if you don't have one yet:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union create login --serverless --auth device-flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÄ BERT Fine-Tuning Pipeline  \n",
    "\n",
    "In this section, we'll execute **tasks and workflows** defined in Python under the relevant folders.  \n",
    "\n",
    "üìÇ Navigate to the `tasks` and `workflows` folders to explore the code. If you're following along in a **hosted Jupyter Notebook**, you can view the files by clicking the **folder icon** (usually on the left side of the screen).  \n",
    "\n",
    "### üõ† Workflow Overview  \n",
    "\n",
    "We‚Äôll create an **end-to-end machine learning pipeline** to train a **BERT model for text classification** using the **Iris dataset**. The workflow consists of the following steps:  \n",
    "\n",
    "1. **Download & Preprocess Dataset** üì•  \n",
    "2. **Download Pretrained BERT Model** ü§ñ  \n",
    "3. **Fine-tune BERT Model** üèãÔ∏è‚Äç‚ôÇÔ∏è  \n",
    "4. **Evaluate Model Performance** üìä  \n",
    "5. **Save Model as an Artifact** üíæ *(We‚Äôll serve the model in the next section)*  \n",
    "6. **Run a Prediction on New Test Data** üîç  \n",
    "\n",
    "> **üí° Note:**  \n",
    "> In more complex ML workflows, **data pipelines** are often separate from **model training pipelines**.  \n",
    "> For simplicity, we'll combine them into a single workflow in this example.  \n",
    "\n",
    "### üîé Explore the Code  \n",
    "\n",
    "To view the workflow, navigate to the [`workflows/train_pipeline.py`](workflows/train_pipeline.py) file.  \n",
    "\n",
    "- Look for the **`train_pipeline()`** function‚Äîthis defines the full workflow.  \n",
    "- The workflow **calls tasks** from the [`tasks`](tasks/) folder.  \n",
    "- It also **builds a container image** using [`containers.py`](containers.py).  \n",
    "\n",
    "Once you understand the structure, **run the workflow** and track your pipeline execution with **Union.ai**! üöÄ  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run --remote workflows/train_pipeline.py train_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Serving the Fine-Tuned BERT model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live App Serving (Beta)\n",
    "\n",
    "Union.ai provides a **simple way to serve your models as a live app**, making it easy to interact with your trained model.  \n",
    "\n",
    "In this example, we'll deploy the model using **Streamlit**, which provides a **simple web interface** for running predictions.  \n",
    "\n",
    "\n",
    "üìÇ Check out the following files for the model-serving code:  \n",
    "- [`app.py`](app.py) ‚Äì Defines the **Streamlit-based UI** for interacting with the model.  \n",
    "- [`main.py`](main.py) ‚Äì Handles **loading the model** and serving it via Union.ai.  \n",
    "\n",
    "Deploy the model by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union deploy apps app.py bert-sentiment-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Union platform `Apps` tab to see the status of all apps!\n",
    "\n",
    "Once the app is live, experiment with different inputs and see how your fine-tuned BERT model performs! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Serving\n",
    "\n",
    "Union.ai also provides a way to serve your models in batch mode. This is useful when you have a large number of predictions to make and you want to do them all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union register workflows/batch_inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from union.remote import UnionRemote\n",
    "# Create a remote connection\n",
    "remote = UnionRemote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_container(data):\n",
    "\n",
    "    inputs = {\"texts\": data}\n",
    "\n",
    "    workflow = remote.fetch_workflow(name=\"workflows.batch_inference.batch_inference_workflow\")\n",
    "    execution = remote.execute(workflow, inputs=inputs, wait=True) # wait=True will block until the execution is complete\n",
    "\n",
    "    # print(execution.outputs)\n",
    "\n",
    "    return execution.outputs[\"o0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_with_container([\"I love this movie\",\n",
    "                               \"I hate this movie\"]\n",
    "                               ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Faster batch serving with Union Actors\n",
    "\n",
    "Union [Actors](https://docs.union.ai/serverless/user-guide/core-concepts/actors/#actors) dramatically reduce the cost of cold starts by maintaining long-running stateful environments that stay ready for use until a defined time-to-live (TTL). This persistent setup eliminates redundant initialization and unlocks several key benefits. This can be especially useful for AI pipelines that benefit from long-running environments, such as large containers, serving models,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_actor(data):\n",
    "\n",
    "    inputs = {\"texts\": data}\n",
    "\n",
    "    workflow = remote.fetch_workflow(name=\"workflows.batch_inference.actor_batch_inference_workflow\")\n",
    "    execution = remote.execute(workflow, inputs=inputs, wait=True) # wait=True will block until the execution is complete\n",
    "\n",
    "    # print(execution.outputs)\n",
    "\n",
    "    return execution.outputs['o0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_with_actor([\"I love this movie\",\n",
    "                               \"I hate this movie\"]\n",
    "                               ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_with_actor([\"I love this movie\",\n",
    "                               \"I hate this movie\"]\n",
    "                               ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_with_actor([\"I love this movie\",\n",
    "                               \"I hate this movie\"]\n",
    "                               ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert-hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
