{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ Fine-Tune and Serve a BERT Model (LLM)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/unionai-oss/bert-llm-classification-pipeline/blob/main/tutorial.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This tutorial will walk you through building an end-to-end Large Language Model (LLM) fine-tuning pipeline using Hugging Face Transformers and Union's AI workflow and inference platform. We'll download a dataset, fine-tune a BERT model for classification on unstructured data, serve the model for inference, and track the pipeline artifacts using Union's powerful MLOps features. Although this example may seem relatively simple, all the concepts and tools used here can be applied to more complex machine learning and AI projects.\n",
    "\n",
    "\n",
    "By just adding a few lines of code to your Python functions, you'll be able to create a reproducible ML pipeline, taking advantage of Union's features:\n",
    "\n",
    "- Reproducible AI workflows: Ensure your ML pipeline produces the same environments every time.\n",
    "- Versioning of code and artifacts: Track changes in your code and models automatically.\n",
    "- Data Caching for faster iterations: Reuse results from previous executions to save time.\n",
    "- Declarative Infrastructure: Define your ML infrastructure needs directly in your code without worrying about provisioning.\n",
    "- Artifact Management for models and data: Automatically manage your model files and datasets.\n",
    "- Container Image Builder: Build and deploy your code in a consistent environment.\n",
    "- Local Development: Test your workflows locally before deploying them to the cloud.\n",
    "- Actors for long-running stateful containers: Handle tasks that require continuous state or interaction.\n",
    "- And more...\n",
    "\n",
    "Example of how to use Union's Python SDK to define a simple ML pipeline:\n",
    "\n",
    "```python\n",
    "@task(\n",
    "    cache=True,\n",
    "    cache_version=\"4\",\n",
    "    container_image=image,\n",
    "    requests=Resources(cpu=\"2\", mem=\"2Gi\")\n",
    ")\n",
    "def download_data(): -> pd.DataFrame:\n",
    "    ...\n",
    "\n",
    "@task(\n",
    "    container_image=image,\n",
    "    requests=Resources(cpu=\"2\", mem=\"20Gi\", gpu=\"1\")\n",
    ")\n",
    "def train_model(data: pd.DataFrame:): -> pytorch.Model:\n",
    "    ...\n",
    "\n",
    "@workflow()\n",
    "def pipeline_workflow():\n",
    "    data = download_data()\n",
    "    train_model(data=data)\n",
    "    ...\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## üß∞ Setup \n",
    "\n",
    "Sign up for a Union Serverless account at [Union.ai](https://union.ai) by clicking the \"Get Started\" button. No card required, and you'll get $30 in free credits to get started. Signing up can take a few minutes.\n",
    "\n",
    "Or you can use your [Union BYOC Enterprise](https://www.union.ai/pricing) login if you have one.\n",
    "\n",
    "### üì¶ Install Python Packages & Clone Repo\n",
    "\n",
    "Packages can be installed in your local environment using the following command using your preferred package manager from the [requirements.txt](requirements.txt) file. For example `pip install -r requirements.txt`. \n",
    "\n",
    "to clone the repo, run the following command in your environment: `git clone `\n",
    "\n",
    "If you're running this notebook in a Google Colab environment, you can install the packages and clone the GitHub repo directly in the notebook by running the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/unionai-oss/bert-llm-classification-pipeline\n",
    "    %cd bert-llm-classification-pipeline\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### üîê Authenticate\n",
    "\n",
    "If you're using [Union BYOC Enterprise](https://www.union.ai/pricing) use: `union create login --host <union-host-url>`\n",
    "\n",
    "Otherwise, Authenticate to [Union Serverless](https://www.union.ai/) by running the command below - create an account for free at [Union.ai](https://union.ai) if you don't have one:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union create login --serverless --auth device-flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÄ BERT Fine-Tuning Pipeline\n",
    "\n",
    "In this sections we'll be running tasks and workflows defined in Python under the relevant folders. \n",
    "\n",
    "Navigate to the `tasks` and `workflows` folders to see the code. if you're following along in a hosted jupyter notebook you should be able to view the code by clicking on a folder icon (usually on the left side of the screen).\n",
    "\n",
    "First we'll create a machine learning pipeline that trains a model on the iris dataset.\n",
    "\n",
    "Our workflow will have the following steps:\n",
    "- Load the iris dataset\n",
    "- Split the dataset into training and testing sets\n",
    "- Train a Random Forest model\n",
    "- Evaluate the model\n",
    "- Save model as an artifact\n",
    "- run a prediction with new data\n",
    "\n",
    "Note: Data pipelines could be seperate from model training pipelines for more complex pipelines. In this example we'll keep it simple and combine them into one workflow.\n",
    "\n",
    "navigate to the [workflows/workflows.py](workflows/workflows.py`workflows.py) file. Find `train_iris_classification()` function to see the code for the workflow. This workflow uses tasks defined in the [/tasks](tasks/data.py) folder and builds a container image from [container.py](containers.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run --remote workflows/train_pipeline.py train_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Serving the Fine-Tuned BERT model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live App Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union deploy apps app.py bert-sentiment-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster batch serving with Union Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
